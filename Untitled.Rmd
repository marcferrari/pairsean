---
title: "Module 4 Pair Quiz"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
#### Ayuson, Marc Edison
#### See, Sean Jacob
#### 7/29/21

## Questions {.tabset .tabfade}

### Question 1

**1. An article in the Journal of Sound and Vibration ["Measurement of Noise-Evoked Blood Pressure by Means of Averaging Method: Relation between Blood Pressure Rise and PSL" (1991, Vol. 151(3), pp. 383-394)] described a study investigating the relationship between noise exposure and hypertension. The following data are representative of those reported in the article.**

---

#### A. Draw a scatter diagram of y (blood pressure rise in millimeters of mercury) versus x (sound pressure level in decibels). Does a simple linear regression model seem reasonable in this situation?

```{r echo=FALSE}
y <- c(1, 0, 1, 2, 5, 1, 4, 6, 2, 3, 5, 4, 6, 8, 4, 5, 7, 9, 7, 6)
x <- c(60,	63,	65,	70,	70,	70,	80,	90,	80,	80,	85,	89,	90,	90,	90,	90,	94,	100,	100,	100)
table<-data.frame(y,x)
knitr::kable(table,"pipe",col.name=c("Blood Pressure Rise in milimeters of Mercury (mmHg)", "Sound Pressure Level in Decibels(dB)"), align=c("c","c"))
```

```{r echo=FALSE}
y <- c(1, 0, 1, 2, 5, 1, 4, 6, 2, 3, 5, 4, 6, 8, 4, 5, 7, 9, 7, 6)
x <- c(60,	63,	65,	70,	70,	70,	80,	90,	80,	80,	85,	89,	90,	90,	90,	90,	94,	100,	100,	100)
plot(x, y, pch=16, cex=1, main = "Sound Pressure Level(dB) vs Blood Pressure Rise(mmHg)" ,xlab = "Sound Pressure Level in Decibels (dB)", ylab= "Blood Pressure Rise in Millimeters of Mercury (mmHg)")
lm(y ~ x)
abline(lm(y ~ x), col = "blue")
```

Based on the graph shown above, **a linear regression model would be reasonable** as there is a linear relationship between the given variables. There is a trend that shows that as one variable increases, the other variable also increases.

---

#### B. Fit the simple linear regression model using least squares. Find an estimate of $\sigma^2$.

```{r}
fit <- lm(y ~ x)
```

We can find the estimate through the formula:

<center>

$\sigma^2 = \frac {SSE}{n-2}$

</center>

wherein n = 20 and,

The formula for the SSE is as follows:

<center>

$\sum_{i=1}^n (y_i-\hat y_i)^2$

</center>

This would give us the value for SSE below:

```{r}
SSE <- sum((fitted(fit) - y)^2)
SSE
```

We can now plug it in the first equation which gives us:

<center>

$\sigma^2 = \frac {31.26647}{20-2}$

$\sigma^2 = \frac {31.26647}{18}$

$\sigma^2 = 1.737026$

</center>

This would give us an **estimated value of $\sigma^2$ as 1.737026**.

#### Rechecking:

```{r}
summary(fit)
```

Shown above is the summary of the fit variable wherein the variable is comprised of the lm function of y and x. With this, **we can also find an estimate of $\sigma^2$ through getting the square of the residual standard error**. This is shown by the code below.

<br>

```{r}
(summary(fit)$sigma)**2 
```

This would give us an **estimated value of 1.737026 for $\sigma^2$**.

---

#### C. Find the predicted mean rise in blood pressure level associated with a sound pressure level of 85 decibels.

The formula for the a simple regression line would be as follows:

<center>

$\hat y = \hat \beta_0 + \hat \beta_1x$

</center>

With this in mind, we can find $\hat \beta_0$ and $\hat \beta_1$ through these steps. 

```{r}
Sumxy = sum((x - mean(x)) * (y - mean(y)))
Sumx2= sum((x - mean(x)) ^ 2)
Sumy2 = sum((y - mean(y)) ^ 2)
c(Sumxy, Sumx2, Sumy2)
```

Through these values, we can already find the values for our first equation.

```{r}
beta1 = Sumxy / Sumx2
beta0 = mean(y) - beta1 * mean(x)
c(beta0, beta1)
```

This gives us the equation:

<center>

$\hat y = -10.1315377 + 0.1742939x$

</center>

This would be our simple linear regression model wherein we can plug the value of which is 85.

<center>

$\hat y = -10.1315377 + 0.1742939(85)$

$\hat y = -10.1315377 + 14.8149815$

$\hat y = 4.6834438$

</center>

Our answer would be 4.6834438 or approximately 5. This would show that our **predicted mean rise in blood pressure level associated with a sound pressure level is 5 mmHg**.



### Question 2

**1. An article in Optical Engineering ["Operating Curve Extraction of a Correlator's Filter" (2004, Vol. 43, pp. 2775-2779)] reported on the use of an optical correlator to perform an experiment by varying brightness and contrast. The resulting modulation is characterized by the useful range of gray levels.**

---

```{r echo=FALSE}
y_i  <- c(96, 50,	50,	112,	96,	80,	155,	144,	255)
x_1 <- c(54,	61,	65,	100,	100,	100,	50,	57,	54)
x_2 <- c(56,	80,	70,	50,	65,	80,	25,	35,	26)

dat<-data.frame(y_i,x_1,x_2)

knitr::kable(dat,"pipe",col.name=c("Useful Range $y_i$", "Brightness (%) $x_{1i}$", "Contrast (%) $x_{2i}$"), align=c("c","c", "c"))

plot(dat)


```



#### A. Fit a multiple linear regression model to these data.

Our working formula for the linear regression model is:

<center>

$\hat y = \beta_0   +   \beta_1x_1   +  \beta_2x_2$

</center>

Through the lm function, we can find the coefficients ($\beta_0 , \beta_1 , \beta_2$) of our linear regression model.

```{r}
B <-	c(54, 61,	65,	100, 100,	100, 50, 57, 54)
C <- c(56, 80, 70, 50, 65, 80, 25, 35, 26)
Ur <- c(96, 50, 50,	112, 96, 80, 155, 144, 255)
lm(Ur ~ B + C)
```

With this, **our linear regression model is $\hat y = 238.5569   +    0.3339x_1   +  (-2.7167)x_2$**.



---

#### B. Estimate $\sigma^2$.

The formula for the error of squares is

$$SS_E=\sum_{i=1}^{n}e_i^2=\sum_{i=1}^{n}(y_i-\hat{y}_i)^2$$



Since the Linear Regression Model is $\hat y = 238.5569   +    0.3339x_1   +  (-2.7167)x_2$, we can solve for $\hat{y_i}$ $SS_E$ using the formula $(y_i-\hat{y_i})^2$

|Number| $y_i$ | $\hat{y_i}$ | $e_i=y_i-\hat{y_i}$ | $e_i^2$ |
|:----:|:-----:|:-----------:|:---------------------:|:-------:|
|     1|     96|     104.4523|                -8.4523|  71.44138|
|     2|     50|      41.5888|                 8.4112|  70.74829|
|     3|     50|      70.0914|               -20.0914|  403.6644|
|     4|    112|     139.1119|               -24.1119|  581.3837|
|     5|     96|      95.3614|                 0.6386|   0.40781|
|     6|     80|      54.6109|                25.3891|  644.6064|
|     7|    155|     187.3344|               -32.3344|  1045.513|
|     8|    144|     162.5047|               -18.5047|  342.4239|
|     9|    255|     185.9533|                69.0467|  4767.447|
|------|-------|-------------|-----------------------|----------|
|$\sum_1^9e_i^2$| | | | 7927.636|




Given: 

$n=9$

$p=3$ 

An unbiased estimator of $\sigma^2$ is

$$\sigma^2=\frac{SS_E}{n-3}$$
where $n$ is the number of samples and subtracted by $3$ because it is the number of regressions + 1 $(df=n-(k+1))$ for the degrees of freedom of the Residual Error 

Inputting the values,

$$\sigma^2=\frac{7927.636}{9-3}$$
$$\sigma^2=1321.273$$

```{r}
lmOut <- lm(y_i ~ x_1 + x_2, data = dat)
summary(lmOut)
```

As we can see in the summary using r, the Residual Standard Error is $36.35$, by squaring it we get the value of $\sigma^2\approx35.35^2\approx1321.3$ 


The estimated $\sigma^2$ is approximately $1321.3$.

---

#### C. Compute the standard errors of the regression coefficients.

Matrix Approach to Multiple Linear Regression

Using the formula for the least squares estimates, we can find the standard errors of the regression coefficients using the Matrix Approach.

$\hat\beta=(X'X)^-1X'y$

where $X$ is 
```{r echo=FALSE}
x_matrix <- c (1, 54, 56, 1,61, 80, 1, 65, 70, 1, 100, 50, 1, 100, 65, 1, 100, 80, 1, 50, 25, 1, 57, 35, 1, 54, 26)
X <- matrix (x_matrix, nrow=9, ncol=3, byrow=TRUE)
X
```
$$\begin{bmatrix}
1 & 54 & 56\\
1 & 61 & 80\\
1 & 65 & 70\\
1 & 100 & 50\\
1 & 100 & 65\\
1 & 100 & 80\\
1 & 50 & 25\\
1 & 57 & 35\\
1 & 54 & 26\\
\end{bmatrix}$$

and $y$ is 

```{r echo=FALSE}
y_matrix <- c (96, 50, 50, 112, 96, 80, 155, 144, 255)
y_mat <- matrix (y_matrix, nrow=9, ncol=3, byrow=TRUE)
y_mat
```
$$\begin{bmatrix}
96\\
50\\
50\\
112\\
96\\
80\\
155\\
144\\
255\\
\end{bmatrix}$$

Getting the matrix of $X'$

```{r echo=FALSE}
x_inv <- t(X)
x_inv
```
$$\begin{bmatrix}
1 & 1 & 1& 1& 1& 1& 1& 1 & 1 \\
54 &61& 65& 100& 100 &100 &50 &57& 54\\
96& 50& 50 &112& 96& 80& 115& 144 &255 
\end{bmatrix}$$

---

#### D. Predict the useful range when brightness = 80 and contrast = 75.



---

#### E. Test for significance of regression using $\alpha$=0.05. What is the P-value for this test?


---

#### F. Construct a t-test on each regression coefficient. What conclusions can you draw about the variables in this model? Use $\alpha$=0.05.



---

##

#### References:

Montgomery, D. C., & Runger, G. C. (2019). 10 Statistical Inference for Two Samples. In Applied Statistics and Probability for Engineers (7th ed., pp. 262–279). essay, Wiley.

R. Pruim, “Mathematics in R Markdown”, Rpruim.github.io, 2021. [Online]. Available: https://rpruim.github.io/s341/S19/from-class/MathinRmd.html. [Accessed: 25- Jul- 2021].

https://www.cyclismo.org/tutorial/R/linearLeastSquares.html

https://daviddalpiaz.github.io/appliedstats/simple-linear-regression.html

https://www.colorado.edu/amath/sites/default/files/attached-files/ch12_0.pdf
